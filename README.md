# üß† Transformers from Scratch

This repository contains an implementation of attention mechanisms and transformer architectures built entirely from scratch using PyTorch. The goal is to demystify the inner workings of transformers by building them piece by piece, without relying on high-level abstractions.

Keep the Paper [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762) by Side.
<br>Implementation is from the paper.


---

## Current Progress

- 11-04-2025 : Embedding & Positional Encodings

## üöÄ What‚Äôs Included by the End

- Scaled Dot-Product Attention
- Multi-Head Attention
- Positional Encoding
- Transformer Encoder Block
- Transformer Decoder Block (optional)
- Vision Transformer (ViT) [coming soon]
- Self-contained training scripts
- Example usage on toy datasets (e.g., classification)

---

## üõ†Ô∏è Technologies

- Python 3.x
- PyTorch
- Jupyter Notebooks / .py scripts

---

## üìñ Sources & References

These resources inspired and guided this project:

- [The Annotated Transformer (Harvard NLP)](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer by Jay Alammar](https://jalammar.github.io/illustrated-transformer/)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)

---

## üß™ Usage

Clone the repository:

```bash
git clone https://github.com/your-username/transformers-from-scratch.git
cd transformers-from-scratch
